{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8418bb",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e57533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e483a7a5",
   "metadata": {},
   "source": [
    "# Part 2: Gathering Links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d4400",
   "metadata": {},
   "source": [
    "In order to extract article links across multiple pages and regions, I have defined three functions: **article_links** which extract all article links on a paticular page, **pages_in_section** which determines the number of pages in a section, and **article_links_in_section** which uses **article_links** and **pages_in_section** to extract all article links in a section. **article_links** and **pages_in_section** takes a url as a absolute path, but **article_links_in_section** takes the name of a section as a string, that is, a relative path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe655bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_links(url):\n",
    "    # Extract HTML content\n",
    "    contents = requests.get(url).text\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    # Extract tags containing the attribute 'type' with value 'article'\n",
    "    articles = soup.find_all(type='article')\n",
    "    # Extract link for each article\n",
    "    links = []\n",
    "    for article in articles:\n",
    "        links.append(article.find(href=True)['href'])\n",
    "    \n",
    "    return links\n",
    "\n",
    "# print(article_links('https://www.bbc.com/news/world/europe'))\n",
    "\n",
    "def pages_in_section(url):\n",
    "    # Initialisation:\n",
    "    # Make a request for first page\n",
    "    n = 1\n",
    "    r = requests.get(url + '?page=' + str(n))\n",
    "    # Get first response status code\n",
    "    s = r.status_code\n",
    "    \n",
    "    while s == 200:\n",
    "        # Make a request for next page\n",
    "        n = n + 1\n",
    "        r = requests.get(url + '?page=' + str(n))\n",
    "        # Get new response status code\n",
    "        s = r.status_code\n",
    "    \n",
    "    return n-1\n",
    "\n",
    "# print(pages_in_section('https://www.bbc.com/news/world/europe'))\n",
    "\n",
    "def article_links_in_section(section):\n",
    "    # Get URL of section\n",
    "    url = 'https://www.bbc.com/news/world/' + section\n",
    "    # Get number of pages in section\n",
    "    num = pages_in_section(url)\n",
    "    # Get all articles on each page\n",
    "    links = []\n",
    "    for i in range(1, num+1):\n",
    "        links = links + article_links(url + '?page=' + str(i))\n",
    "    \n",
    "    return links\n",
    "\n",
    "# print(article_links_in_section('europe'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c380b4",
   "metadata": {},
   "source": [
    "Now, let's write a little piece of code which extract all article links across all sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993b2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4796\n"
     ]
    }
   ],
   "source": [
    "sections = [\n",
    "    'africa',\n",
    "    'asia',\n",
    "    'australia',\n",
    "    'europe',\n",
    "    'latin_america',\n",
    "    'middle_east'\n",
    "    ]\n",
    "\n",
    "links = []\n",
    "for section in sections:\n",
    "    links += article_links_in_section(section)\n",
    "\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65680fee",
   "metadata": {},
   "source": [
    "As can be seen from the last line of the code, we've extracted around 5000 article links.\n",
    "\n",
    "Now, let's export the links to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60418516",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'source': links}\n",
    "\n",
    "dataframe = pd.DataFrame(dic)\n",
    "\n",
    "dataframe.to_csv('all_article_links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed09f9",
   "metadata": {},
   "source": [
    "# Part 3: Scraping Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7080e7",
   "metadata": {},
   "source": [
    "I have defined three functions: The first function is **meta_content** which tries to extract date, headline and author. It uses a scheme which works for most links, however, there are a few where it isn't able extract date, headline or author. The second function is **text_content** which will extract the relevant paragraphs that make up the main content of the article. Lastly, the third function is **article** which uses uses **meta_content** and **text_content** to extract date, headline, author (if it can) and the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea368b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_content(soup):\n",
    "    # Extract meta content of article, if it exist according to scheme\n",
    "    try:\n",
    "        meta = soup.find('script', type='application/ld+json').text\n",
    "    except Exception:\n",
    "        return ['None', 'None', 'None']\n",
    "    # Convert string to dictionary\n",
    "    meta = json.loads(meta)\n",
    "    # Extract date, headline and author, if they exist according to scheme\n",
    "    lst = []\n",
    "    try:\n",
    "        lst.append(meta['datePublished'][:10])\n",
    "    except Exception:\n",
    "        lst.append('None')\n",
    "    try:\n",
    "        lst.append(meta['headline'])\n",
    "    except Exception:\n",
    "        lst.append('None')\n",
    "    try:\n",
    "        lst.append(meta['author'][0]['name'][3:])\n",
    "    except Exception:\n",
    "        lst.append('None')\n",
    "    return lst\n",
    "\n",
    "def text_content(soup):\n",
    "    # Extract tags containing relevant paragraphs from article\n",
    "    paragraphs = soup.find_all(attrs = {'data-component': 'text-block'})\n",
    "    # Extract text from paragraphs\n",
    "    text = ''\n",
    "    for paragraph in paragraphs:\n",
    "        text = text + paragraph.text + ' '\n",
    "    return text\n",
    "\n",
    "def article(url):\n",
    "    article = {'date': '', 'headline': '', 'author': '', 'text': ''}\n",
    "    # Extract HTML content\n",
    "    contents = requests.get(url).text\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    \n",
    "    # Add date, headline and author to dictionary\n",
    "    meta = meta_content(soup)\n",
    "    article['date']     = meta[0]\n",
    "    article['headline'] = meta[1]\n",
    "    article['author']   = meta[2]\n",
    "    \n",
    "    # Add text to dictionary\n",
    "    text = text_content(soup)\n",
    "    article['text'] = text\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e232ad",
   "metadata": {},
   "source": [
    "Let's try to scrape the first article link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29199046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2024-03-07', 'headline': 'Kuriga kidnap: More than 280 Nigerian pupils abducted', 'author': 'Mansur Abubakar', 'text': 'More than 280 Nigerian school pupils have been abducted in the north-western town of Kuriga, officials say.  The pupils were in the assembly ground around 08:30 (07:30 GMT) when dozens of gunmen on motorcycles rode through the school, one witness said. The students, between the ages of eight and 15, were taken away, along with a teacher, they added. Kidnap gangs, known as bandits, have seized hundreds of people in recent years, especially the north-west. However, there had been a reduction in the mass abduction of children over the past year until this week. Those kidnapped are usually freed after a ransom is paid. The mass abduction was confirmed by Uba Sani, the governor of Kaduna state, which includes Kuriga. He said 187 students had gone missing from a secondary school and 125 from the local primary school but that 25 had since returned. The eyewitness said that one girl had been shot by the gunmen and was receiving medical attention at the Birnin Gwari hospital. A teacher who managed to escape said local people had tried to rescue the children, but they were repelled by the gunmen and one person was killed.  Zakariyya Nasiru, who had a brother and sister taken hostage, told the BBC the family were unable to sleep on Thursday night.\"All of us couldn\\'t sleep as we keep thinking about them. We are here praying for their safe return.\"Mr Nasiru said one boy had escaped last night and had brought back harrowing reports of their conditions, including a lack of food. Almost every family in the town is thought to have a child among those kidnapped. The armed forces have launched an operation to find them. \"No child will be left behind,\" vowed the governor. In January, bandits killed a school principal in the area and abducted his wife. The kidnapping comes days after dozens of women and children were feared kidnapped by the Boko Haram Islamist group while they were collecting firewood in north-eastern Nigeria. However, the two cases of mass abductions are not thought to be related. The criminal kidnap gangs that bring fear to north-western Nigeria are separate to the militant Islamist group Boko Haram in the north-east, although there have been reports that they may have worked together on occasion. Thursday\\'s attack happened in an area controlled by Ansaru, a breakaway faction of Boko Haram, which kidnapped more than 200 schoolgirls from the town of Chibok in 2014. In an attempt to curb Nigeria\\'s spiralling and lucrative kidnapping industry, a controversial law that has made it a crime to make ransom payments was passed in 2022. It carries a jail sentence of at least 15 years, however no-one has ever been arrested. Earlier this year, the family of a group of sisters kidnapped in the capital, Abuja, denied a police statement that the security forces had rescued the girls, saying that they had no choice but to pay the ransom. '}\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.bbc.com'\n",
    "\n",
    "print(article(url + links[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13584749",
   "metadata": {},
   "source": [
    "... it seems to do the job in this case.\n",
    "\n",
    "One could scrape all the articles with the following piece of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "articles = []\n",
    "for link in links:\n",
    "    articles.append(article(url + link))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54de1f2",
   "metadata": {},
   "source": [
    "Due to time constraints, let's slice the list of links..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86912f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = links[0:3]\n",
    "\n",
    "articles = []\n",
    "for link in links:\n",
    "    articles.append(article(url + link))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6de5205",
   "metadata": {},
   "source": [
    "And we can easily export it to a csv file as we did with the article links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "023205c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'articles': articles}\n",
    "\n",
    "dataframe = pd.DataFrame(dic)\n",
    "\n",
    "dataframe.to_csv('all_articles.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
